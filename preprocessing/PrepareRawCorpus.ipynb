{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import os\n",
    "import seaborn as sb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listdir = [r.split('.')[0] for r in os.listdir('input_raw/')]\n",
    "corpus = []\n",
    "def remove_blank_space_batch(batch):\n",
    "    results = []\n",
    "    for lst in batch:\n",
    "        result = [item for item in lst if item != '']\n",
    "        results.append(result)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for filename in listdir:\n",
    "#     filename = \"robotics\"\n",
    "    #     print filename\n",
    "    dat = pd.read_csv(\"input_raw/%s.csv\"%filename)\n",
    "    dat.dropna(how='any',inplace=True)\n",
    "    remove_blank_space = lambda d: str(d).rstrip().replace('\\n','')\n",
    "    dat['title'] = dat['title'].map(remove_blank_space)\n",
    "    dat['content'] = dat['content'].map(remove_blank_space)\n",
    "    # dat['doc'] = dat['title'] +' '+dat['content'] + ' ' + dat['tags']\n",
    "    dat['doc'] = dat['title'] +' '+dat['content']\n",
    "\n",
    "    def get_trigram(dat):\n",
    "            sentences = [nltk.word_tokenize(w) for w in dat]\n",
    "            bigram = Phrases(sentences,delimiter='-')\n",
    "            trigram = Phrases(bigram[sentences],delimiter='-')\n",
    "    #         quadgram = Phrases(trigram[sentences],delimiter='-')\n",
    "    #         pentagram = Phrases(quadgram[sentences],delimiter='-')\n",
    "            return trigram\n",
    "\n",
    "    trigram = get_trigram(dat['doc'])\n",
    "    sentences = [trigram[sent.rstrip().split(' ')] for sent in dat['doc']]\n",
    "    corpus.append(remove_blank_space_batch(sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " # Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality\n",
    "min_word_count = 40   # Minimum word count\n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size\n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "print \"Training Word2Vec model...\"\n",
    "model = Word2Vec(sentences, workers=num_workers,\n",
    "            size=num_features, min_count = min_word_count,\n",
    "            window = context, sample = downsampling, seed=1)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# It can be helpful to create a meaningful model name and\n",
    "# save the model for later use. You can load it later using Word2Vec.load()\n",
    "model_name = \"w2v_model\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
